---
layout: post
---

梯度下降(gradient descent)是一种最优化算法，基于爬山法的搜索策略，其原理简单易懂，广泛应用于机器学习和各种神经网络模型中。在吴恩达的神经网络课程中，梯度下降算法是最先拿来教学的基础算法。

##梯度下降算法的基本原理
要想找到某函数的最大值或者最小值，最迅速的方法就是沿着梯度的方向上升或者下降。梯度下降算法就是以目标的负梯度方向对参数进行调整，沿着目标函数J(x,y)的梯度（一阶偏导数）的相反方向，即−∇θJ(x,y)来不断更新参数直到达到目标函数的极值点。当然除了方向还应该有下降的步长即学习率(learning rate)，一般这个参数是提前指定好。
基本的梯度下降算法无法满足众多应用需求，所以每一个先进的机器学习框架或者模型都会使用梯度下降的变种，目前广泛使用的有三种，分别是全量梯度下降(batch gradient descent)、随机梯度下降(stochastic gradient descent)和批量梯度(mini-batch gradient descent)下降。
###全量梯度下降(batch gradient descent)
又称Full Batch Learning，顾名思义该方法是由整个训练集来确定梯度的方向，需要将所有的训练数据都计算一遍。公式如下：
`θ=θ−η⋅∇θJ(θ)`
优点和缺点也是显而易见的。优点就是遍历全部数据得到的梯度方向是相对来说最正确的，因此能更好的找到极值点。缺点是一次计算大量的数据需要非常大的内存，而且计算时间过长。数据集比较大的时候不适合这种方法。
### 随机梯度下降(stochastic gradient descent)
随机梯度下降是根据每个样本来计算梯度决定方向，即计算一个样本就更新一次参数，公式如下：
`θ=θ−η⋅∇θJ(θ;x(i);y(i))`
相对于全量梯度下降来说，随即梯度下降的一个明显优势就是速度更快，而且可以在线学习。但是由单个样本来确定梯度方向会出现一定的随机性，如图所示，想要达到极值点，SGD需要更多的迭代次数。
![SGD](http://images2015.cnblogs.com/blog/764050/201512/764050-20151230193523495-665207012.png)
但是更新的随机性又一个明显的好处，就是有可能跳出局部最优点，进而找到另一个结果更好的局部最优甚至找到全剧最优，当然这只能是启发式的想法，很难得到验证。
###批量梯度下降(mini-batch gradient descent)
以上两种方法可以说是比较极端的情况，都有各自的局限和优点，那么有没有一种方法可以中和一下两家的特点呢？于是批量梯度下降就出现了。公式如下：
`θ=θ−η⋅∇θJ(θ;x(i:i+n);y(i:i+n))`
因为是选取了一部分样本计算梯度，所以相对于全量梯度下降效率更快，相对于随机梯度下降来说随机性较少，既提升了效率又保证了整体的稳定性。是目前应用较为广泛的一种梯度下降算法。
其实这三种方法基本类似，唯一的区别就是batch-size的大小，当batch-size=1时即为随机梯度下降。batch-size=trainset-size时即为全量梯度下降。因此在训练时如何选择一个合适的batch的大小也是一个对结果有影响的trick。
https://www.zhihu.com/question/32673260 在这个知乎问答里提到了batch-size的设置问题，具体还是要根据自己的实验多调整，找到自己数据的规律。
##总结
以上是最基本的梯度下降算法原理介绍，在此基础上已经衍生出了更多的高阶优化算法，各机器学习框架中也已经集成了各种方法，如adam等，以后的日子再更新这些高阶方法的分析。