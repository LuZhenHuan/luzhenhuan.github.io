<?xml version="1.0" encoding="utf-8"?>
<search>
  
    
    <entry>
      <title><![CDATA[使用t-SNE可视化高维数据]]></title>
      <url>%2F2017%2F03%2F08%2F%E4%BD%BF%E7%94%A8t-SNE%E5%8F%AF%E8%A7%86%E5%8C%96%E6%95%B0%E6%8D%AE%2F</url>
      <content type="text"><![CDATA[在机器学习的问题中，高维变量是非常常见的，他们对于机器来讲很容易计算，但是对于人来说，看起来就非常痛苦了。所以，有很多给数据降维的手段，比如PCA，LDA等。t-SNE也是其中的一种，但它属于非线性降维，所以t-SNE降维的主要作用貌似只能用来数据可视化和分析数据，并不能进一步作为feature来使用。但瑕不掩瑜，它的可视化效果是非常好的，学习起来也并不难。 t-SNE是从SNE演化来的，他们都采用了将空间中的距离转换为概率的方式来表示点与点之间的相似度，然后求高维空间和低维空间的概率分布的KL Divergence作为两个分布的距离，希望最小化这个距离函数。SNE中对于高维和低维空间都使用高斯分布来建模，但是会造成“拥挤问题”，也就是最后低维空间中的数据点会聚集在一起。所以t-SNE使用了t分布来定义低维空间中点的相似性。具体的原理在Laurens van der Maaten的视频讲解里讲的特别清楚，可以花半个多小时的时间看一下。另外，这篇中文博文讲的也不错, 还给出了手工实现的代码，可以学习一个。 但是这里我们并不打算自己造轮子，Python的scikit-learn包已经支持了t-SNE工具，很方便调用，使用近似算法可以将复杂度降低到O(nlogn)，下面我们用这个工具对MNIST数据集进行一个可视化的实验，来看看它究竟效果如何。 首先，import相关的包，并且加载MNIST数据集： import numpy as np import matplotlib.pyplot as plt from time import time from sklearn import datasets, manifold 然后，我们先定义好画图的函数，使之能将二维的embedding显示出来： # Scale and visualize the embedding vectors def plot_embedding(X, title=None): x_min, x_max = np.min(X, 0), np.max(X, 0) X = (X - x_min) / (x_max - x_min) plt.figure() ax = plt.subplot(111) for i in range(X.shape[0]): plt.text(X[i, 0], X[i, 1], str(digits.target[i]), color=plt.cm.Set1(y[i] / 10.), fontdict={&apos;weight&apos;: &apos;bold&apos;, &apos;size&apos;: 9}) plt.xticks([]), plt.yticks([]) if title is not None: plt.title(title) 最后，我们调用t-SNE工具包，将数据降维后进行绘制： # t-SNE embedding of the digits dataset print(&quot;Computing t-SNE embedding&quot;) tsne = manifold.TSNE(n_components=2, init=&apos;pca&apos;, random_state=0) t0 = time() X_tsne = tsne.fit_transform(X) plot_embedding(X_tsne, &quot;t-SNE embedding of the digits (time %.2fs)&quot; % (time() - t0)) plt.show() 绘制的图表如下： 可以看到，对于1083个64维的数据点，共花费了6.39秒的时间进行训练，时间还是很慢的。但是，分类的效果特别好，相同的digit基本都聚集在了一起。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[在Python中使用Stanford CoreNLP进行数据预处理]]></title>
      <url>%2F2017%2F02%2F20%2F%E5%9C%A8Python%E4%B8%AD%E4%BD%BF%E7%94%A8Stanford-CoreNLP%E8%BF%9B%E8%A1%8C%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86%2F</url>
      <content type="text"><![CDATA[Stanford CoreNLP工具包Stanford CoreNLP是自然语言处理领域非常重要并且好用的工具包, 它集成了tokenization, lemmatization, pos tagging, parsing等基础工具，也包括情感分析、命名实体识别等高级功能。而且，在某些任务上，该工具包包含对于中文在内的多种语言，因此在很多文章中都可以看到使用该工具来进行文本的预处理。 Stanford NLP group为该工具编写了很完善的使用文档，因此本文将不着重于介绍它的功能，而重点说明如何在Python中调用该工具包。实际上，Stanford Corenlp提供了jar包，在下载之后可以直接通过命令行的方式运行，但这一方式很慢，在处理多个文件时，对于每一个文件程序都会花费大量的时间来重新加载模型，所以，我们希望模型只需要加载一次，这里我们要用到的就是它的Server模式。 使用Server模式首先，在官网下载该工具包。解压后，cd进入目录。通过如下命令，运行该工具包的Server模式： 1java -mx4g -cp &quot;*&quot; edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 15000 之后，程序会监听指定端口（9000），对于我们post到该端口的请求进行处理。运行截图如下： 在Python中发送请求在Python中可以直接使用requests对相应端口进行post请求，请求的方式如下图所示： 需要注意的是，需要的annotator和返回的格式需要在url中指定，post的数据只包含原始文本。CoreNLP所支持的所有annotator可见官方的介绍。 除了直接请求外，多个Python工具包封装了对Stanford CoreNLP的请求，我采用的是pycorenlp,它的使用方式非常简单，可以直接通过pip安装： 1pip install pycorenlp 使用时，先import该python包： 12&gt;&gt;&gt; from pycorenlp import StanfordCoreNLP&gt;&gt;&gt; nlp = StanfordCoreNLP(&apos;http://localhost:9000&apos;) 之后，对文本进行处理： 1234567891011121314151617&gt;&gt;&gt; text = ( &apos;Pusheen and Smitha walked along the beach. &apos; &apos;Pusheen wanted to surf, but fell off the surfboard.&apos;)&gt;&gt;&gt; output = nlp.annotate(text, properties=&#123; &apos;annotators&apos;: &apos;tokenize,ssplit,pos,depparse,parse&apos;, &apos;outputFormat&apos;: &apos;json&apos; &#125;)&gt;&gt;&gt; print(output[&apos;sentences&apos;][0][&apos;parse&apos;])(ROOT (S (NP (NNP Pusheen) (CC and) (NNP Smitha)) (VP (VBD walked) (PP (IN along) (NP (DT the) (NN beach)))) (. .))) 当然，也可以指定更复杂的annotator，这里不一一说明。对于多语言，Stanford CoreNLP支持的annotator如下表所示：]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Ubuntu 16.04 配置 Tensorflow 深度学习环境]]></title>
      <url>%2F2016%2F11%2F30%2FUbuntu-16-04-%E9%85%8D%E7%BD%AE-Tensorflow-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83%2F</url>
      <content type="text"><![CDATA[最近，在实验室的主机上装好了深度学习的环境，因为配置步骤比较复杂，所以在这里记录一下，一是为了以后有问题的时候参考，二是可以给大家配置环境的时候提供一个借鉴。 我们配置的深度学习环境如下： 显卡： GeForce GTX 1080 * 2 Nvidia 显卡驱动 367.57 Cuda Tookit 8.0 cuDNN v5 Tensorflow 0.12 下面开始进入安装步骤： 显卡驱动安装过程需要在终端模式下进行(Desktop按Ctrl+Alt+F1 切换到终端界面) 驱动安装文件下载从 Nvidia 官网选择并下载显卡对应的驱动安装文件，一般为 NVIDIA-Linux-x86_64_version_num.run 卸载旧驱动，禁用自带驱动等 卸载可能存在的旧版本 nvidia 驱动 1$sudo apt-get remove --purge nvidia* 安装驱动可能需要的依赖(可选) 12$sudo apt-get update$sudo apt-get install dkms build-essential linux-headers-generic 把 nouveau 驱动加入黑名单 1$sudo nano /etc/modprobe.d/blacklist-nouveau.conf 加入如下内容：12345blacklist nouveaublacklist lbm-nouveauoptions nouveau modeset=0alias nouveau offalias lbm-nouveau off 禁用 nouveau 内核模块 12$echo options nouveau modeset=0 | sudo tee -a /etc/modprobe.d/nouveau-kms.conf$sudo update-initramfs -u 重启计算机 1sudo reboot 运行驱动安装文件并重启1sudo sh NVIDIA-Linux-x86_64_version_num.run 安装Cuda Toolkit关闭桌面管理系统在关闭桌面管理 lightdm 的情况下安装驱动似乎可以实现Intel 核芯显卡 来显示 + NVIDIA 显卡来计算，参考: 1sudo service lightdm stop 下载安装包并安转从官网下载相应的安装包，并按照提示安装即可。 我们选择了安转deb包，通过以下命令进行安装： 1234sudo dpkg -i cuda-repo-&lt;distro&gt;_&lt;version&gt;_&lt;architecture&gt;.debsudo apt updatesudo apt install cudasudo reboot 安装cuDNN下载与复制从官网下载最新的安装包，解压后将文件复制到对应的位置： 1234tar xvzf cudnn-8.0-linux-x64-v5.1-ga.tgzsudo cp cuda/include/cudnn.h /usr/local/cuda/includesudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64sudo chmod a+r /usr/local/cuda/include/cudnn.h /usr/local/cuda/lib64/libcudnn* 配置环境变量：打开~/.bashrc文件，在末尾添加以下内容，注意nvidia-367版本号可能会有不同：123export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/lib/nvidia-367export CUDA_HOME=/usr/local/cudaexport PATH=$PATH:/usr/local/cuda/bin 安装tensorflow目前, tensorflow 0.12已经支持pip安装，可以直接参考官方教程进行。 到这里，整个环境就配置好了，可以运行ipython之后import tensorflow进行测试，也可以通过跑tensorflow源码中的例子进行测试，比如tensorflow/tensorflow/models/image/mnist中的CNN模型，在这里不再详述。 参考： 官方文档：https://www.tensorflow.org/versions/r0.12/get_started/ Install tensorflow with cuda: http://city.shaform.com/blog/2016/10/31/install-tensorflow-with-cuda.html Ubuntu16.04 英伟达显卡驱动：https://gist.github.com/dangbiao1991/7825db1d17df9231f4101f034ecd5a2b]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[使用Makefile来编译Tex文件]]></title>
      <url>%2F2016%2F11%2F29%2F%E4%BD%BF%E7%94%A8Makefile%E6%9D%A5%E7%BC%96%E8%AF%91Tex%E6%96%87%E4%BB%B6%2F</url>
      <content type="text"><![CDATA[最近在新配置的Linux机器上面写论文，因为只是小的改动，不想用Texmaker之类的IDE。而且因为IDE会产生很多的依赖和log文件，用dropbox同步的时候比较烦人。所以，转向了命令行来编译tex文件。当然，每次都敲latex，biblatex，pdflatex还是很烦人的，于是自己写了一个Makefile，这样，通过make命令就可以编译和查看文章了。 首先，需要安装texlive，也就是整个tex包： 1sudo apt-get install texlive 然后，在新建好的tex目录下，新建Makefile文件，文件内容如下： 12345678910111213141516171819202122232425262728293031323334353637filename=主tex文件名，比如paperall: pdf clean readpdf: ps ps2pdf $&#123;filename&#125;.pspdf-print: ps ps2pdf -dColorConversionStrategy=/LeaveColorUnchanged -dPDFSETTINGS=/printer $&#123;filename&#125;.pstext: html html2text -width 100 -style pretty $&#123;filename&#125;/$&#123;filename&#125;.html | sed -n &apos;/./,$$p&apos; | head -n-2 &gt;$&#123;filename&#125;.txthtml: @#latex2html -split +0 -info &quot;&quot; -no_navigation $&#123;filename&#125; htlatex $&#123;filename&#125;ps: dvi dvips -t letter $&#123;filename&#125;.dvidvi: latex $&#123;filename&#125; bibtex $&#123;filename&#125; latex $&#123;filename&#125; latex $&#123;filename&#125;read: evince $&#123;filename&#125;.pdf &amp;aread: acroread $&#123;filename&#125;.pdfclean: rm -f *.toc *.aux *.log *.out *.blg *.bbl *.dvi *.ps *.gzcleanall: rm -f $&#123;filename&#125;.pdf *.toc *.aux *.log *.out *.blg *.bbl *.dvi *.ps *.gz 其中, all: pdf clean read 表示当执行make命令时，程序将会先后执行pdf、clean、read这三个命令对应的内容。如果不需要clean和read则可以删掉。 之后，编译tex和查看pdf只需要执行：1make 如果想删除所有的log文件，可执行：1make clean 如果想连pdf文件也一起删除，可执行：1make cleanall]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Several Methods to Go Over GFW]]></title>
      <url>%2F2016%2F11%2F17%2FSeveral-Methods-to-Go-Over-GFW%2F</url>
      <content type="text"><![CDATA[Years ago, I started to climb over GFW and embrace a brand new Internet world. It’s fantac. Even though there are many unreal rumors about China, I still find large amounts of useful information and Google definitely improved my efficiency. There are various of ways to fuck GFW. I used to use shadowsocks but now I turn to hosts and IPV6. So I’d like to make a summary of these methods and I would continally add more methods if I find them really useful. Github Education PackageGithub education package contains dozens of software to help students work and study. Most of all, it supplies a $50 credits for digital ocean, which can be used to set up your vps. According to my test, the Sanfrancisco hosts and Singapore hosts are faster than others in China. VPNThis is a classic way to go over GFW. You can set up PPTP, L2TP or IPSec VPN servers on your own vps. Or you can buy one account. Here are some famous websites for this: 云梯 Recommendations from other website. LanternSome people recommend this months ago. I didn’t try it but they said it’s fascinating. You can explore it at its official website. And this is a blog for reference. ShadowsocksShadowsocks is now very popular in China because of its security and all-platform support. I used it for nearly two years on my Android phone and some of my servers. It’s easy for a programmer or handyman to set up a shadowsocks server on his own vps. You can follow the steps from this blog. It’s a complete tutorial. HostsThese days I found this approach quite fascinating in its speed and fast configuration. Here I recommend two well-maintained repositories for use: IPV4 hosts IPV6 hosts For education net user, I greatly recommend you to use the IPV6 version or combine them together. For ordinary users, the first one also works well. DNS serverThis is a alternative method to Hosts. But I should emphasize here it’s not stable. Since some dns requests will be detected by GFW and GFW will return you with a dummy ip address. Also, sometimes the dns servers may get polluted. However, despite its unstability, this method works in my iPhone/iPad device, since you cannot change the hosts file without rooting the device. Good dns servers are listed here: Google IPV4 DNS: 8.8.8.8 nad 8.8.4.4 Google IPV6 DNS: 2001:4860:4860::8888 and 2001:4860:4860::8844 OpenDNS IPV4 and IPV6 mixed DNS (IPV6 first): 2620:0:ccc::2 and 2620:0:ccd::2 To summarize, if you are in education net, like myself, I greatly suggest the Hosts method, because you can take advantage of IPV6 and the access is much faster. And for Iphone/Ipad device with out root, you can use the IPV6 dns server. Even though it’s not very stable, I find most of useful websites are always available.]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[New Workstation for Deep Learning]]></title>
      <url>%2F2016%2F11%2F13%2FNew-Workstation-for-Deep-Learning%2F</url>
      <content type="text"><![CDATA[Our lab is equiped with a new workstation recently. I spent some time studying how to assembly such a computer for deep learning and configure the deep learning environment. Finally I made it and here is a picture of the inner architecure. Quite cool, aha? We have one Intel 6850K CPU and two GTX 1080 gpu cards in it, which I think is enough for our small lab to do some deep learning experiments in NLP. Of course, we don’t want to compete with other big group in the Neural Machine Learning field. And now my work space is like this: I have to say it’s quite comfortable and I even want to sleep here when the cold winter comes!]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[New start]]></title>
      <url>%2F2016%2F11%2F12%2FNew-start%2F</url>
      <content type="text"><![CDATA[After the first two headache months of my graduate study, I’m ready to re-launch this blog today. I’m going to write some of my technical practices, research strucggles and interesting daily stories here. Hopefully I can continue this good habit to write blogs sometimes. And thanks for your visiting and hope you can enjoy something here. Now let’s start!]]></content>
    </entry>

    
  
  
</search>
